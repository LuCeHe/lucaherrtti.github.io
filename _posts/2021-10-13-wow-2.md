---
layout: post
title: A chatbot to talk about the real stuff [Ep. 2].
published: true
comments: true
---

I wanted to complete the [previous post](https://lucehe.github.io/wow/) where I reproduced in tensorflow one of the generative chatbots
 proposed by 
[Wizard of Wikipedia: Knowledge-Powered Conversational agents](https://arxiv.org/pdf/1811.01241.pdf). Funny enough, they
were ok linking my project in their [official repository](https://parl.ai/projects/wizard_of_wikipedia/) 
(check the *Note: an unofficial ...*), which could not make me more proud! There's three things I wanted to show:

1. my results on the complete table 4 of the paper
2. my results when the network doesn't use the knowledge base at all, which are remarkably good
3. show a few samples of a dialogue


<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-fymr{border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}
</style>
<table class="tg"  style="border:1px solid black;margin-left:auto;margin-right:auto;">
<thead>
  <tr>
    <th class="tg-0pky"></th>
    <th class="tg-0pky" colspan="4">Predicted Knowledge</th>
    <th class="tg-0pky" colspan="4">Gold Knowledge</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky"></td>
    <td class="tg-0pky" colspan="2">Seen</td>
    <td class="tg-0pky" colspan="2">Unseen</td>
    <td class="tg-0pky" colspan="2">Seen</td>
    <td class="tg-0pky" colspan="2">Unseen</td>
  </tr>
  <tr>
    <td class="tg-0pky"></td>
    <td class="tg-0pky">PPL</td>
    <td class="tg-0pky">F1</td>
    <td class="tg-0pky">PPL</td>
    <td class="tg-0pky">F1</td>
    <td class="tg-0pky">PPL</td>
    <td class="tg-0pky">F1</td>
    <td class="tg-0pky">PPL</td>
    <td class="tg-0pky">F1</td>
  </tr>
  <tr>
    <td class="tg-0pky">E2E (original)</td>
    <td class="tg-0pky">63.5   </td>
    <td class="tg-0pky">16.9</td>
    <td class="tg-0pky">97.3</td>
    <td class="tg-0pky">14.4</td>
    <td class="tg-0pky">23.1</td>
    <td class="tg-fymr">35.5</td>
    <td class="tg-0pky">32.8</td>
    <td class="tg-fymr">32.2</td>
  </tr>
  <tr>
    <td class="tg-0pky">E2E (ours)</td>
    <td class="tg-fymr">12.4</td>
    <td class="tg-fymr">30.6</td>
    <td class="tg-fymr">18.7</td>
    <td class="tg-fymr">25.6</td>
    <td class="tg-fymr">11.7</td>
    <td class="tg-0pky">31.6</td>
    <td class="tg-fymr">17.5</td>
    <td class="tg-0pky">26.3</td>
  </tr>
  <tr>
    <td class="tg-0pky">E2E-nk (ours)</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">13.0</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
  </tr>
</tbody>
</table>

PPL stands for Perplexity and the lower the better, while for F1, the higher the better.
Our naive reimplementation happens to outperform the original in all the data splits
used in the original work, based on PPL, while in terms of F1 seems to be more sensitive
on the seed chosen for training in our experience. 
Very interestingly, in the case when the knowledge to use is predicted by the
network (Predicted Knowledge) and not provided by the task (Gold Knowledge), our implementation
closes the gap massively with respect to the original implementation. 

We were curious to know if the network was really using the knowledge base, so we implemented 
a GPT variant that has no encoder. The PPL achieved is surprisingly low, indicating that PPL 
doesn't capture clarly to which extent the knowledge base is being used. For that reason
it would be appropriate to use the metric the original article proposed, Wiki F1, to compare
the dialogue generated directly with the knowledge base, and probably extending Wiki F1 to 
Wiki BLEU, to consider longer n-grams, could be a clever idea.

{% if page.comments %} 



<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-lucehe-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



{% endif %}
