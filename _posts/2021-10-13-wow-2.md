---
layout: post
title: A chatbot to talk about the real stuff [Ep. 2].
published: true
comments: true
---

I wanted to complete the [previous post](https://lucehe.github.io/wow/) where I reproduced in tensorflow one of the generative chatbots
 proposed by 
[Wizard of Wikipedia: Knowledge-Powered Conversational agents](https://arxiv.org/pdf/1811.01241.pdf). Funny enough, they
were ok linking my project in their [official repository](https://parl.ai/projects/wizard_of_wikipedia/) 
(check the *Note: an unofficial ...*), which could not make me more proud! 

There's three things I wanted to show:

1. my results on the complete table 4 of the paper
2. my results when the network doesn't use the knowledge base at all, which are remarkably good
3. show a few samples of a dialogue


<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-fymr{border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}
</style>
<table class="tg"  style="border:1px solid black;margin-left:auto;margin-right:auto;">
<thead>
  <tr>
    <th class="tg-0pky"></th>
    <th class="tg-0pky" colspan="4">Predicted Knowledge</th>
    <th class="tg-0pky" colspan="4">Gold Knowledge</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky"></td>
    <td class="tg-0pky" colspan="2">Seen</td>
    <td class="tg-0pky" colspan="2">Unseen</td>
    <td class="tg-0pky" colspan="2">Seen</td>
    <td class="tg-0pky" colspan="2">Unseen</td>
  </tr>
  <tr>
    <td class="tg-0pky"></td>
    <td class="tg-0pky">PPL</td>
    <td class="tg-0pky">F1</td>
    <td class="tg-0pky">PPL</td>
    <td class="tg-0pky">F1</td>
    <td class="tg-0pky">PPL</td>
    <td class="tg-0pky">F1</td>
    <td class="tg-0pky">PPL</td>
    <td class="tg-0pky">F1</td>
  </tr>
  <tr>
    <td class="tg-0pky">E2E (original)</td>
    <td class="tg-0pky">63.5   </td>
    <td class="tg-0pky">16.9</td>
    <td class="tg-0pky">97.3</td>
    <td class="tg-0pky">14.4</td>
    <td class="tg-0pky">23.1</td>
    <td class="tg-fymr">35.5</td>
    <td class="tg-0pky">32.8</td>
    <td class="tg-fymr">32.2</td>
  </tr>
  <tr>
    <td class="tg-0pky">E2E (ours)</td>
    <td class="tg-fymr">12.4</td>
    <td class="tg-fymr">30.6</td>
    <td class="tg-fymr">18.7</td>
    <td class="tg-fymr">25.6</td>
    <td class="tg-fymr">11.7</td>
    <td class="tg-0pky">31.6</td>
    <td class="tg-fymr">17.5</td>
    <td class="tg-0pky">26.3</td>
  </tr>
  <tr>
    <td class="tg-0pky">E2E-nk (ours)</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">13.0</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
  </tr>
</tbody>
</table>

PPL stands for Perplexity and the lower the better, while for F1, the higher the better.
Our naive reimplementation happens to outperform the original in all the data splits
used in the original work, based on PPL, while in terms of F1 seems to be more sensitive
on the seed chosen for training in our experience. 
Very interestingly, in the case when the knowledge to use is predicted by the
network (Predicted Knowledge) and not provided by the task (Gold Knowledge), our implementation
closes the gap massively with respect to the original implementation. 

We were curious to know if the network was really using the knowledge base, so we implemented 
a GPT variant of the original implementation, with analog hyperparameters, that has no encoder 
(E2E-nk in the table). The PPL achieved is surprisingly low, indicating that PPL 
doesn't capture clearly to which extent the knowledge base is being used. For that reason
it would be appropriate to use the metric the original article proposed, Wiki F1, to compare
the dialogue generated directly with the knowledge base, and probably, extending Wiki F1 to 
Wiki BLEU, to consider longer n-grams, could be a clever idea.

And an example dialogue. Bare in mind that the replies the network recieves were collected
in human interactions, so, they don't react to the generation of the network, and the network
replies are conditioned on the human dialogue, so, not on his own replies. I think I'm going
to write a code to be able to interact directly with the chatbot, but probably for an Episode 3:

Topic: Gardening 
human generation: I like Gard ening , even when I ' ve only been doing it for a short time .
model generation:  I love gard ening , it is a relax ing activity
context:           Gard ening you : I like Gard ening , even when I ' ve only been doing it for a short time . me : I live on a farm , we garden all year long , it is very relax ing . you : That sounds great . I ' ve always thought that I would love living in a farm , but I ; ve always lived in the city . What do you mostly plant ?
model generation:  I love to garden and cultiv ating plants .
context:           Gard ening , even when I ' ve only been doing it for a short time . me : I live on a farm , we garden all year long , it is very relax ing . you : That sounds great . I ' ve always thought that I would love living in a farm , but I ; ve always lived in the city . What do you mostly plant ? me : I have planted several fruits trees , tom atoes , j al ep enos , bell pep pers , on ions , Gar lic , and potatoes mostly . you : Great , I love the idea of growing my own vegetables and fruits ! Do you have animals in the farm ?
model generation:  I ' m not sure , but I do know that there are over a hundred different types of plants .
context:           ve always lived in the city . What do you mostly plant ? me : I have planted several fruits trees , tom atoes , j al ep enos , bell pep pers , on ions , Gar lic , and potatoes mostly . you : Great , I love the idea of growing my own vegetables and fruits ! Do you have animals in the farm ? me : yes i do . C ows , chick ens , Micro pigs , Gu in ne as , We also do forest growing also . we plants large pine trees . you : W ow , it sounds amazing , the Micro - pigs are so cute ! are they train able to be well behav ed ?
model generation:  Yes , I do . I like to grow a lot of different types of plants .





{% if page.comments %} 



<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-lucehe-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



{% endif %}
