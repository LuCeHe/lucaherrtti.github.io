---
layout: post
title: A chatbot to talk about the real stuff [Ep. 2].
published: true
comments: true
---

I wanted to complete the [previous post](https://lucehe.github.io/wow/) where I reproduced in tensorflow one of the generative chatbots
 proposed by 
[Wizard of Wikipedia: Knowledge-Powered Conversational agents](https://arxiv.org/pdf/1811.01241.pdf). Funny enough, they
were ok linking my project in their [official repository](https://parl.ai/projects/wizard_of_wikipedia/) 
(check the *Note: an unofficial ...*), which could not make me more proud! 
As well I mentioned in the previous post that I didn't know if they optimized on the
perplexity (PPL) or masked perplexity, and they confirmed that they optimized on
PPL, which is good news, since my results are better on PPL.

There's three things I wanted to show:

1. my results on the complete table 4 of the paper
2. my results when the network doesn't use the knowledge base at all, which are remarkably good
3. show a few samples of a dialogue


<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-fymr{border-color:inherit;font-weight:bold;text-align:left;vertical-align:top}
</style>
<table class="tg"  style="border:1px solid black;margin-left:auto;margin-right:auto;">
<thead>
  <tr>
    <th class="tg-0pky"></th>
    <th class="tg-0pky" colspan="4">Predicted Knowledge</th>
    <th class="tg-0pky" colspan="4">Gold Knowledge</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky"></td>
    <td class="tg-0pky" colspan="2">Seen</td>
    <td class="tg-0pky" colspan="2">Unseen</td>
    <td class="tg-0pky" colspan="2">Seen</td>
    <td class="tg-0pky" colspan="2">Unseen</td>
  </tr>
  <tr>
    <td class="tg-0pky"></td>
    <td class="tg-0pky">PPL</td>
    <td class="tg-0pky">F1</td>
    <td class="tg-0pky">PPL</td>
    <td class="tg-0pky">F1</td>
    <td class="tg-0pky">PPL</td>
    <td class="tg-0pky">F1</td>
    <td class="tg-0pky">PPL</td>
    <td class="tg-0pky">F1</td>
  </tr>
  <tr>
    <td class="tg-0pky">E2E (original)</td>
    <td class="tg-0pky">63.5   </td>
    <td class="tg-0pky">16.9</td>
    <td class="tg-0pky">97.3</td>
    <td class="tg-0pky">14.4</td>
    <td class="tg-0pky">23.1</td>
    <td class="tg-fymr">35.5</td>
    <td class="tg-0pky">32.8</td>
    <td class="tg-fymr">32.2</td>
  </tr>
  <tr>
    <td class="tg-0pky">E2E (ours)</td>
    <td class="tg-fymr">12.4</td>
    <td class="tg-fymr">30.6</td>
    <td class="tg-fymr">18.7</td>
    <td class="tg-fymr">25.6</td>
    <td class="tg-fymr">11.7</td>
    <td class="tg-0pky">31.6</td>
    <td class="tg-fymr">17.5</td>
    <td class="tg-0pky">26.3</td>
  </tr>
  <tr>
    <td class="tg-0pky">E2E-nk (ours)</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">13.0</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0pky">-</td>
  </tr>
</tbody>
</table>

PPL stands for Perplexity and the lower the better, while for F1, the higher the better.
Our naive reimplementation happens to outperform the original in all the data splits
used in the original work, based on PPL, while in terms of F1 seems to be more sensitive
on the seed chosen for training in our experience. 
Very interestingly, in the case when the knowledge to use is predicted by the
network (Predicted Knowledge) and not provided by the task (Gold Knowledge), our implementation
closes the gap massively with respect to the original implementation. That means that our
knowledge dropout, since I think the major difference with the original lies there, allows
the network to better predict what is the piece of information necessary to reply appropriately 
to the querier.

We were curious to know if the network was really using the knowledge base, so we implemented 
a GPT variant of the original implementation, with analog hyperparameters, that has no encoder 
(E2E-nk in the table). The PPL achieved is surprisingly low, indicating that PPL 
doesn't capture clearly to which extent the knowledge base is being used. For that reason
it would be appropriate to use one of the  metrics the original article proposed, Wiki F1, to compare
the dialogue generated directly with the knowledge base, and probably, extending Wiki F1 to 
Wiki BLEU, to consider longer n-grams, could be a clever idea.

And an example dialogue. Bare in mind that the replies the network recieves were collected
in human interactions, so, they don't react to the generation of the network, and the network
replies are conditioned on the human dialogue, so, not on its own replies. 

### Topic: Gardening

**human:** I like Gardening, even when I've only been doing it for a short time.

**model:** I love gardening, it is a relaxing activity

**human:** That sounds great. I've always thought that I would love living in a farm, but I've always lived in the city. What do you mostly plant?

**model:** I love to garden and cultivating plants.

**human:** Great, I love the idea of growing my own vegetables and fruits! Do you have animals in the farm?

**model:** I'm not sure, but I do know that there are over a hundred different types of plants.

**human:** Wow, it sounds amazing, the Micro - pigs are so cute! are they trainable to be well behaved?

**model:**  Yes, I do. I like to grow a lot of different types of plants.


Well, we see some issues. Even if the perplexities are good, language sounds a 
bit generic and it does not seem to react properly to the history of the conversation,
even considering the limitations mentioned above. I want to code a proper interaction
with the model, to give a better impression on how it would feel to talk directly 
with the model. Probably for an episode 3!





{% if page.comments %} 



<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-lucehe-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



{% endif %}
