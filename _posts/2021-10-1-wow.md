---
layout: post
title: A chatbot to talk about the real stuff .
published: true
comments: true
---


Have you ever wondered how it is to talk to an AI that can answer to precise 
questions about life? 
Well we wonder too, and we don't have the answer for you tonight. 
What we did is to translate into 
[tensorflow](https://github.com/LuCeHe/lucehe.github.io/blob/master/_codes_for_tutorials/2021-10-1-wow.py) 
what can be defined as a grounded chatbot, that was originally written in in 
[pytorch](https://github.com/facebookresearch/ParlAI/tree/main/projects/wizard_of_wikipedia) for 
[ParlAI](https://github.com/facebookresearch/ParlAI). 

The original paper 
[Wizard of Wikipedia: Knowledge-Powered Conversational agents](https://arxiv.org/pdf/1811.01241.pdf),
introduces a dataset where two humans were asked to play one the role of a Wikipedia
expert (the Wizard), the other the role of an apprentice (the Apprentice). They are meant 
to have an open ended dialogue, where the Wizard has access to Wikipedia articles 
to be able to give more accurate replies. The goal is to have an AI learning to 
play the role of the expert. So, they introduce two baseline architecture based on 
[Transformer](https://arxiv.org/pdf/1706.03762.pdf), that they name End2End and Two-Stage.

<img src="/images/wow_architecture.png" alt="wow_architecture" class="center">

We only translated into tensorflow the End2End variant. The best performances we achieve are a validation perplexity of 11.3 and a 
validation F1 of 35.7%, which is a small improvement over the results reported by the 
original work, table 4, of a test perplexity of 23.1 and test F1 of 35.5. In coming 
iterations of this post I will compare test results on test results, but I didn't 
want to overfit on the test set, so I didn't record it.

A small ambiguity remains, since we computed a variant of perplexity that doesn't 
take into account masked symbols, see padding symbol. The performance is a bit 
worse on validation masked perplexity, a minimum of 54.0, but the original 
pytorch code, doesn't seem to use a masked perplexity. Here the training curves
with some beautiful overfitting going on:

<img src="/images/histories_wow.png" alt="histories_wow" class="center">



More details to come. I plan to improve this post little by little, so I can 
showcase some tricks that I've learned, and hopefully I can make it more
didactical with further iterations. I wrote the code to be able to make small 
variants that I want to publish, but since now I have the baseline code that I 
wrote from scratch and gives slightly better accuracy, kind of proud to show.




{% if page.comments %} 



<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://https-lucehe-github-io.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



{% endif %}
